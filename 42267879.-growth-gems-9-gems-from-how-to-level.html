<p>Hi there,<br><br>Today's gems are mined from another <a href="https://youtu.be/ojFnN265fEM">presentation made during the App Promotion Summit by&nbsp;Karan Tibdewal</a> (Growth Consultant at Phiture):<br></p><h2>How to Level Up Your A/B Testing Game</h2><p><br><strong>ğŸ’#1</strong><br><br>To get to the point where more testing = more growth being true you need to have a thorough process in place. <br><br><a href="https://youtu.be/ojFnN265fEM?t=808">13:28</a></p><div><hr></div><p><strong>ğŸ’#2</strong><br><br>3 key goals of the testing framework are:</p><ol><li><p>Communicate ideas from all teams and prioritize based on company objectives</p></li><li><p>Understand and define leading and lagging metrics</p></li><li><p>Centralize test results and communicate next steps <br><br><a href="https://youtu.be/ojFnN265fEM?t=852">14:12</a></p></li></ol><div><hr></div><p><strong>ğŸ’#3</strong><br><br>Use slides for your experiments: excel files are good for internal use but slides are better because they help get the buy-in from different teams and get them involved. <br><br><a href="https://youtu.be/ojFnN265fEM?t=852">17:13</a></p><div><hr></div><p><strong>ğŸ’#4</strong><br><br>When it comes to goal metrics, you do not want to only optimize for revenue: you want "leading metrics" that feed in to revenue because there are a lot of variables that go into optimizing for revenue. <br><br><a href="https://youtu.be/ojFnN265fEM?t=1368">22:48</a></p><div><hr></div><p><strong>ğŸ’#5</strong><br><br>To know which ideas to prioritize, measure the Impact score with Impact = Reach x Relevance x Frequency. <br><br><a href="https://youtu.be/ojFnN265fEM?t=1455">24:15</a></p><div><hr></div><p><strong>ğŸ’#6<br></strong><br>Set up experiments for success by planning ahead of time and using a sample size calculator tool like the one from Optimizely (<a href="https://www.optimizely.com/sample-size-calculator/">link here</a>). <br><br><a href="https://youtu.be/ojFnN265fEM?t=1707">28:27</a></p><div><hr></div><p><strong>ğŸ’#7</strong><br><br>You should not have a "win or lose" mentality. Once you have a significant positive impact you want to scale up the test (apply to 100%) then you want to start iterating on it. Double down on high impact tests and keep iterating. <br><br><a href="https://youtu.be/ojFnN265fEM?t=1880">31:20</a></p><div><hr></div><p><strong>ğŸ’#8</strong><br><br>After 2 or 3 variations of a test idea that does not prove a significant positive impact, do not get too attached to your idea and stop iterating: it is not meant to be. <br><br><a href="https://youtu.be/ojFnN265fEM?t=1925">32:05</a></p><div><hr></div><p><strong>ğŸ’#9</strong><br><br>Do a reach audit where you assess how many tests you could run with your user base with your current engagement rate. Example: don't just look at how many people will receive that email and assume a 10% conversion rate. Instead, look at your past performance, look at how many users you can reach and use calculator to figure out time to reach statistical significance. <br><br><a href="https://youtu.be/ojFnN265fEM?t=2150">35:50</a></p><div><hr></div><p><strong>ğŸ’#10</strong><br><br>Onboarding and activation funnels are the main areas of improvement for almost all of the apps (even 5+ years apps!). <br><br><a href="https://youtu.be/ojFnN265fEM?t=2180">36:20</a></p><div><hr></div><p><strong>ğŸ’#11</strong><br><br>How long to run tests for? A week is often enough but it of course depends on volume. Usually the absolute difference between variants should be more than 200-300 to get proper test results (if you are very small). Do not go for tests that are more than 1-1.5 months. <br><br><a href="https://youtu.be/ojFnN265fEM?t=2995">49:55</a></p><div><hr></div><p><strong>ğŸ’#12</strong><br><br>How often do you revisit experiments? Once you have an experiment in place and you have maximized/iterated on it, it is good to have quarterly reviews. <br><br><a href="https://youtu.be/ojFnN265fEM?t=3152">52:32</a></p><div><hr></div><p>Stay savvy!</p><p>â›ï¸ Sylvain</p>